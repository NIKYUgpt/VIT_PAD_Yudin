{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjYGhY5byzLZ"
   },
   "source": [
    "# Препроцессинг данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToKpETQw6QNO"
   },
   "source": [
    "Загружаем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4b4gaQNBJjQ",
    "outputId": "b15f1d31-fb7e-4828-ab64-fff22468e2c8"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "!pip install pymorphy2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9IRn81GBJjR"
   },
   "source": [
    "# Sentiment Analysis - Анализ тональности\n",
    "\n",
    "Сегодня мы познакомимся с основами NLP на примере задачи анализа тональность (Sentiment Analysis) с соревнования [Kaggle Competition](https://www.kaggle.com/c/sentiment-analysis-in-russian/data) .\n",
    "\n",
    "Задача соревнования - построить ML модель, способную различать тональность (позитивная, негативная, нейтральная) новостей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFLw2o-zBJjS"
   },
   "source": [
    "## Загружаем данные\n",
    "\n",
    "Данные записаны в формате `json`. Для его чтения воспользуемся библиотекой `json` и методом `open`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdpKqJiPncFE",
    "outputId": "dd01b377-c2ad-41b8-d0fa-3fa669ddbc26"
   },
   "outputs": [],
   "source": [
    "!unzip kazakh_news.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oF-J7rimBJjV"
   },
   "outputs": [],
   "source": [
    "# Загружаем данные\n",
    "with open('train.json', encoding = 'utf-8') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNZlUXTn0Oos"
   },
   "source": [
    "Будем решать задачу классификации на 3 класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4BBZhvydBz1E",
    "outputId": "9be11412-9c6e-4495-865b-56a61166201f"
   },
   "outputs": [],
   "source": [
    "print(set([x[\"sentiment\"] for x in data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CICDzVha0WSJ"
   },
   "source": [
    "Каждый пример для обучения состоит из id, текстового фрагмента новости, и лейбла (`'positive', 'negative', 'neutral'`).\n",
    "\n",
    "Задача - предсказать лейбл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EoTLqGuBBJjX",
    "outputId": "d1f0a50a-99e3-4c1d-eadd-1cba91de3250"
   },
   "outputs": [],
   "source": [
    "# Посмотрим на пример\n",
    "num = 1\n",
    "\n",
    "print(\"ID: \",          data[num][\"id\"], \"\\n\")\n",
    "print(\"Text: \\n\",      data[num][\"text\"])\n",
    "print(\"Sentiment: \",   data[num][\"sentiment\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O35nZmrIYc_R"
   },
   "source": [
    "Посмотрим на соотношение классов. Видим, что в данных присутсвует сильный дисбаланс. Почти половина всех новостей - нейтральные, а негативных новостей меньше всего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vpyRyuYlYPWU",
    "outputId": "9a8db8c4-8e12-4d81-9e1b-e5a70ba4cab4"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(len(data))\n",
    "Counter([x['sentiment'] for x in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNlcOsYi0rae"
   },
   "source": [
    "# Предобработка данных\n",
    "Прежде, чем перейти к ML, текст необходимо предобработать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LIukr1f3Jwz"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ojF4gM2BJjY"
   },
   "source": [
    "## Шаг 1. Токенизация и удаление стоп-слов\n",
    "Первый шаг предобработки - разбить текст на единицы, с которыми мы будем работать. Эти единицв называются **токенами (tokens)**, а процесс - **токенизация (tokenization)**. В большинстве случаев в качестве токенов используют слова, но иногда работают с буквами.\n",
    "\n",
    "Будем работать со словами. Проще всего разбить текст на слова по пробелам (не забывая про пунктуацию).\n",
    "\n",
    "\n",
    "## Шаг 2. Удаляем стоп-слова\n",
    "\n",
    "\n",
    "**Стоп-слова** – это слова, которые выкидываются из текста при обработке текста. Когда мы применяем машинное обучение к текстам, такие слова могут добавить много шума, поэтому необходимо избавляться от нерелевантных слов.\n",
    "\n",
    "Стоп-слова это обычно понимают артикли, междометия, союзы и т.д., которые не несут смысловой нагрузки. При этом надо понимать, что не существует универсального списка стоп-слов, все зависит от конкретного случая.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YQ8azssBJjY"
   },
   "outputs": [],
   "source": [
    "import nltk   # Natural Language Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tU6aMrY2BJjZ",
    "outputId": "defd8a2c-17ba-4eb8-eaa3-ff863653e2f5"
   },
   "outputs": [],
   "source": [
    "# загружаем список стоп-слов для русского\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('russian')\n",
    "\n",
    "# примеры стоп-слов\n",
    "print(len(stop_words))\n",
    "print(stop_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaOE5jnH6QNg"
   },
   "source": [
    "Инициализируем `WordPunctTokenizer`, с помощью которого затем разобьем текст на слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pwzsbXp6BJjc",
    "outputId": "726e1748-f054-470a-91dc-90dc011a31a8"
   },
   "outputs": [],
   "source": [
    "word_tokenizer = nltk.WordPunctTokenizer()\n",
    "tokens = word_tokenizer.tokenize('казнить нельзя, помиловать!?!!')\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg947Mrc6QNg"
   },
   "source": [
    "Запишем предобработку текста в виде функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "rqm0z-EQ-Cw9",
    "outputId": "1a077294-f6fb-407d-c403-271848901f55"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile(r'[А-Яа-яA-zёЁ-]+')\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    try:\n",
    "        return \" \".join(regex.findall(text)).lower()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "words_only('Казнить, нельзя помиловать!!! 2023 год')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmCdblcaBJjd"
   },
   "outputs": [],
   "source": [
    "# расширим список стоп-слов, словами, которые являеются стоп-словами в данной задаче\n",
    "add_stop_words = ['kz', 'казахстан', 'астана', 'казахский', 'алматы', 'ао', 'оао', 'ооо']\n",
    "months = ['январь', 'февраль', 'март', 'апрель', 'май', 'июнь', 'июль', 'август', 'сентябрь', 'октябрь', 'ноябрь', 'декабрь',]\n",
    "all_stop_words = stop_words + add_stop_words + months\n",
    "\n",
    "def process_data(data):\n",
    "    texts = []\n",
    "    targets = []\n",
    "\n",
    "    # поочередно проходим по всем новостям в списке\n",
    "    for item in tqdm(data):\n",
    "\n",
    "        text_lower = words_only(item['text']) # оставим только слова\n",
    "        tokens     = word_tokenizer.tokenize(text_lower) #разбиваем текст на слова\n",
    "\n",
    "        # удаляем пунктуацию и стоп-слова\n",
    "        tokens = [word for word in tokens if (word not in all_stop_words and not word.isnumeric())]\n",
    "\n",
    "        texts.append(tokens) # добавляем в предобработанный список\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1OPuPlD7BJjd",
    "outputId": "e7325c74-4d6a-4d28-aeff-06620fae5d40"
   },
   "outputs": [],
   "source": [
    "# запускаем нашу предобработку\n",
    "y = [item['sentiment'] for item in data]\n",
    "texts = process_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CorgBzHc6QNi"
   },
   "source": [
    "Теперь каждый пример представлен списком слов. Причем все слова с маленькой буквы. Пунктуацию и стоп-слова мы удалили."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUkC7UlgBJje",
    "outputId": "695cbe5a-57aa-4bb4-a106-74c06f131fe3"
   },
   "outputs": [],
   "source": [
    "# example\n",
    "i = 2\n",
    "print(\"Label: \", y[i])\n",
    "print(\"Tokens: \", texts[i][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7TTICcdOQOZ"
   },
   "source": [
    "## Шаг 3 . Нормализация слов\n",
    "\n",
    "Обычно тексты содержат разные грамматические формы одного и того же слова, а также могут встречаться однокоренные слова. Чтобы унифицировать слова в тексте и избавиться от различных форм слова, слова в тексте можно нормализовать.\n",
    "\n",
    "\n",
    "Существует 2 наиболее известных способа нормализации слов: **стемминг (stemming)** и **лемматизация (лемматизация)**.\n",
    "\n",
    "В общих чертах они похоже, но между этими методами есть различия. В зависимости от языка и задачи тот или иной метод может быть предпочтительнее.\n",
    "\n",
    "1) **Стемминг** (англ. stemming — находить происхождение) — это процесс нахождения основы слова для заданного исходного слова. Основа слова не обязательно совпадает с морфологическим корнем слова и не обязана являться существующим словом в языке. Стемминг – это грубый эвристический процесс, который отрезает «лишнее» от корня слов, часто это приводит к потере словообразовательных суффиксов.\n",
    "\n",
    "2) **Лемматизация** приводит все встречающиеся словоформы к одной, нормальной словарной форме. **Лемматизация** использует словарь и морфологический анализ, чтобы в итоге привести слово к его канонической форме – **лемме**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_qoB5CNOQOZ"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# инициализируем стеммер\n",
    "stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wq2dnoDpOQOZ",
    "outputId": "cfb01249-1587-42c9-eaf3-1dafa1f32571"
   },
   "outputs": [],
   "source": [
    "# примеры стемминга\n",
    "i = 1\n",
    "for aword in texts[i][:10]:\n",
    "    aword_stem = stemmer.stem(aword)\n",
    "    print(\"ДО: %s, ПОСЛЕ: %s\" % (aword, aword_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPIxss9oOQOa",
    "outputId": "de9fa81c-a8fe-4799-de8d-860273601af2"
   },
   "outputs": [],
   "source": [
    "text = 'в этот прекрасный солнечный день мы сидим на семинаре по обработке естественного языка в университете имени Витте'\n",
    "stemmed_text = ' '.join([stemmer.stem(x) for x in text.split(' ')])\n",
    "print('Исходная строка:\\t',text)\n",
    "print('Обрубленная строка:\\t',stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A_uE2Z1RQi3W",
    "outputId": "d133580f-2197-401f-e800-8e9a333286c9"
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "text = 'On this beautiful sunny day we are sitting in a natural language processing seminar at Witte University'\n",
    "stemmed_text = ' '.join([SnowballStemmer(\"english\").stem(x) for x in text.split(' ')])\n",
    "print('Original text:\\t',text)\n",
    "print('Stemmed text:\\t',stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ogIbTw6OQOa"
   },
   "source": [
    "Видно, что для русского языка результат не очень хороший. Попробуем что-нибудь получше.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4dg8BwbBJjg"
   },
   "outputs": [],
   "source": [
    "# загружаем библиотеку для лемматизации\n",
    "import pymorphy2 # Морфологический анализатор\n",
    "\n",
    "# инициализируем лемматизатор :)\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLDccTjU6QNi"
   },
   "source": [
    "Посмотрим на примерах, как работает лемматизация."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Lvqn80spvoY",
    "outputId": "10576b7c-2f2a-41c2-ffbc-a631bc5782a4"
   },
   "outputs": [],
   "source": [
    " morph.parse('студентами')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "[здесь](https://pymorphy2.readthedocs.io/en/stable/user/grammemes.html#grammeme-docs) можно посмотреть список граммем\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0G-MSYZCr78",
    "outputId": "9db2c2c3-f96a-4762-c8c1-a657d4de0d2e"
   },
   "outputs": [],
   "source": [
    " morph.parse('лук')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NaPT-b1fBJjh",
    "outputId": "87a5fa3d-4879-4a8a-f5ec-a3a6823cd612"
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "for aword in texts[i][:10]:\n",
    "    aword_norm = morph.parse(aword)[0].normal_form\n",
    "    print(\"Исходное слово: %s\\tЛемматизированное: %s\" % (aword, aword_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PawKzMNpDEId",
    "outputId": "e20c3572-1746-4dfe-98f8-b7febf1cfb3c"
   },
   "outputs": [],
   "source": [
    "text = 'в этот прекрасный солнечный день мы сидим на семинаре по обработке естественного языка в университете имени Витте'\n",
    "stemmed_text = ' '.join([morph.parse(x)[0].normal_form for x in text.split(' ')])\n",
    "print('Оригинальный текст:\\t',text)\n",
    "print('Лемматизированный текст:\\t',stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFH_5BeYHNKS"
   },
   "source": [
    "Теперь давайте лемматизируем все тексты.\n",
    "\n",
    "## Be careful! Осторожно\n",
    "**Библиотека Pymorphy может работать. Чтобы не ждать вы можете загрухить уже лемматизированный текст**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UFLXWJVoHQV"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# применяем лемматизацию ко всем текстам\n",
    "for i in tqdm_notebook(range(len(texts))):           # tqdm_notebook создает шкалу прогресса :)\n",
    "    text_lemmatized = [morph.parse(x)[0].normal_form for x in texts[i]] # применяем лемматизацию для каждого слова в тексте\n",
    "    texts[i] = ' '.join(text_lemmatized)                # объединяем все слова в одну строку через пробел"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ucT3aNc6QNk"
   },
   "source": [
    "Чтобы не ждать загружаем предобработанные тексты :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ji4lX4JSH_yo"
   },
   "outputs": [],
   "source": [
    "texts = [x.replace('\\n','') for x in open('text_lemmatized.txt', encoding = 'utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-7Svmby6QNl"
   },
   "source": [
    "Посмотрим на пример."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0eWDYSn6BJji",
    "outputId": "d03dc3e4-9fae-4fa5-ea72-3bba783a09cf"
   },
   "outputs": [],
   "source": [
    "# посмотрим на пример\n",
    "i = 123\n",
    "print(\"Label: \",   y[i])\n",
    "print(\"Text: \\n\",  texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLPpW3iR4Xn2"
   },
   "source": [
    "# Моделирование & Векторные представления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rq1t2c-1BJjj"
   },
   "source": [
    "## Разбиваем на train&test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eWUtNl36QNl"
   },
   "source": [
    "Лейблы у нас также закодированы словами. Для корректной работы алгорима конвертируем их в числа (`'negative', 'neutral', 'positive'`):\n",
    "\n",
    "    negative = -1\n",
    "    neutral  = 0\n",
    "    positive = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrhjwhR56QNl"
   },
   "outputs": [],
   "source": [
    "# Функция для кодирования лейблов\n",
    "def label2num(y):\n",
    "    if y == 'positive':\n",
    "        return 1\n",
    "    if y == 'negative':\n",
    "        return -1\n",
    "    if y == 'neutral':\n",
    "        return 0\n",
    "\n",
    "encoded_y = [label2num(yy) for yy in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsEcoPTo6QNm"
   },
   "source": [
    "**отложим часть данных для тестирования и оценки качества алгоритма. Для этого воспользуемся функцией `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGSTMZ-5BJjk"
   },
   "outputs": [],
   "source": [
    "#train test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, test_texts, train_y, test_y = train_test_split(texts, encoded_y, test_size=0.2, random_state=42, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98P2PUQRKJ0u"
   },
   "source": [
    "# Bag of Words\n",
    "\n",
    "**Bag of Words или мешок слов** — это модель, представляющая собой неупорядоченный набор слов, входящих в обрабатываемый текст.\n",
    "\n",
    "\n",
    "Часто модель представляют в виде матрицы, в которой строки соответствуют отдельному тексту, а столбцы — входящие в него слова. Ячейки на пересечении являются числом вхождения данного слова в соответствующий документ. Данная модель удобна тем, что переводит человеческий язык слов в понятный для компьтера язык цифр.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwFUqLL26QNq"
   },
   "source": [
    "Модель Bag of Words реализована в библиотеке `sklearn` в классе `feature_extraction.text.CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9RSOydTZKNuR",
    "outputId": "75229964-b852-4bc8-a4f2-9c12ed93e47e"
   },
   "outputs": [],
   "source": [
    "#Инициализируем векторайзер\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3), max_features = 200)\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "# Топ-10 слов\n",
    "vectorizer.get_feature_names_out()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(vectorizer.vocabulary_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "len(vectorizer.get_feature_names_out())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rCJk94G6QNq"
   },
   "source": [
    "Обучаем `vectorizer` на train-данных и сразу преобразем их в вектора с помощью метода `fit_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AulqBPaIKhWH",
    "outputId": "538cc0bf-16c7-4de5-943d-57769b60e838"
   },
   "outputs": [],
   "source": [
    "# Обучаем vectorizer на train-данных и сразу преобразем их в вектора с помощью метода fit_transform\n",
    "train_X = vectorizer.transform(train_texts)\n",
    "train_X.todense()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38ZN6xrlrFhY",
    "outputId": "c52ccfcf-5d1c-4b67-e5ae-2092bcd1e14a"
   },
   "outputs": [],
   "source": [
    "\n",
    "type(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOz12OP76QNr"
   },
   "source": [
    "Также применяем обученный `vectorizer` к данным для тестирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYPY1e0K6QNr"
   },
   "outputs": [],
   "source": [
    "test_X  = vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsDyfAjF4nBR"
   },
   "source": [
    "Теперь каждое предложение задано в виде вектора! Можем строить классификатор!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yWddQCK6QNs"
   },
   "source": [
    "# Обучаем классификатор.\n",
    "\n",
    "В качестве классификатора будем использовать **Random Forest**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qh2xNZx3OaJB"
   },
   "outputs": [],
   "source": [
    "#import алгоритма из библиотеки\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# инициализируем модель\n",
    "clf = RandomForestClassifier(n_estimators = 500, max_depth = 10)\n",
    "\n",
    "# обучаем ее на тренировочных данных\n",
    "clf = clf.fit(train_X, train_y)\n",
    "\n",
    "# делаем предсказание для тестовых данных\n",
    "pred = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1P4PJQd6QNs",
    "outputId": "a95c4715-2a14-4fc7-acef-c31b220e94ff"
   },
   "outputs": [],
   "source": [
    "print('Предсказанные метки: ', pred[0:20], \".....\")\n",
    "print('Истинные метки: ', test_y[0:20], \".....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syfomGfi6QNt"
   },
   "source": [
    "Ничего не понятно! Приведем метки в нормальный вид!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvMmZ5NX6QNt",
    "outputId": "fc343749-2168-403c-ec04-b0b8f8ad4ba7"
   },
   "outputs": [],
   "source": [
    "# Функция для кодирования лейблов\n",
    "def num2label(y):\n",
    "    if y == 1:\n",
    "        return 'positive'\n",
    "    if y == -1:\n",
    "        return 'negative'\n",
    "    if y == 0:\n",
    "        return 'neutral'\n",
    "\n",
    "decoded_pred = [num2label(y) for y in pred]\n",
    "decoded_test_y = [num2label(y) for y in test_y]\n",
    "print('Предсказанные метки: ', decoded_pred[0:20], \".....\")\n",
    "print('Истинные метки: ', decoded_test_y [0:20], \".....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86r2qDjV6QNt"
   },
   "source": [
    "### Оценка качества\n",
    "\n",
    "Качество классификатора будем оценивать по метрикам accuracy и f1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhXYLfIl6QNu",
    "outputId": "4ec76c90-d89b-4ff3-e4ab-7a6c93a90708"
   },
   "outputs": [],
   "source": [
    "print('Accuracy: ', accuracy_score(test_y, pred))\n",
    "print('Precision: ', precision_score(test_y, pred, average='weighted'))\n",
    "print('Recall: ', recall_score(test_y, pred, average='weighted'))\n",
    "print('F1: ', f1_score(test_y, pred, average = 'weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSo4OBph6QNu"
   },
   "source": [
    "# Посмотрим на несколько примеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bjadFZAy6QNu",
    "outputId": "a1f04824-474d-49a9-c57f-8ff716bc962a"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('Истинный лейбл:',decoded_test_y[i])\n",
    "    print('Предсказанный лейбл:',decoded_pred[i])\n",
    "    print('Текст новости: ', train_texts[i][:500]+'...')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InIuMjRn6QNv"
   },
   "source": [
    "# Bonus: TF-IDF - вектора чуть поумнее\n",
    "\n",
    "**TF-IDF** (от англ. TF — term frequency, IDF — inverse document frequency) — статистическая мера, используемая для оценки важности слова в контексте документа, являющегося частью коллекции документов или корпуса. Вес некоторого слова пропорционален частоте употребления этого слова в документе и обратно пропорционален частоте употребления слова во всех документах коллекции.\n",
    "\n",
    "\n",
    "**Term Frequency** число раз терм $t$ встречается в документе $d$.\n",
    "$$\n",
    "TF_{t,d} = term\\!\\!-\\!\\!frequency(t, d)\n",
    "$$\n",
    "\n",
    "**Inverse Document Frequency** мера того, сколько информации несет данное слово. Иными словами, частотные слова, содержащиеся во всех документах несут мало информации, в то время как слова частотные лишь в ограниченном числе документов содержат большое количество информации об этих документах. **IDF** - это инверсия частоты, с которой некоторое слово встречается в документах коллекции.\n",
    "\n",
    "\n",
    "$$\n",
    "IDF_t = inverse\\!\\!-\\!\\!document\\!\\!-\\!\\!frequency(t) = \\log \\frac{N}{DF_t}\n",
    "$$\n",
    "\n",
    "$N$ - число документов в корпусе.\n",
    "\n",
    "$DF_t$ - число документов содержащих слово $t$.\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "TF\\!\\!-\\!\\!IDF_{t,d} = TF_{t,d} \\times IDF_t\n",
    "$$\n",
    "\n",
    "TF-IDF оценивает важность слов в корпусе документов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSSFiNIaOQOh"
   },
   "source": [
    "Модель TF-IDF реализована в библиотеке `sklearn` в классе `feature_extraction.TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YqpUSs2f6QNv",
    "outputId": "6f2a48c3-893d-4f67-b34f-c91c18b0fc7c"
   },
   "outputs": [],
   "source": [
    "#вычисляем tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Fit TF-IDF on train texts\n",
    "vectorizer = TfidfVectorizer(max_features = 200, norm = None) # возмем топ 200 слов\n",
    "vectorizer.fit(train_texts)\n",
    "\n",
    "# Топ-10 слов\n",
    "vectorizer.get_feature_names_out()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEWdcqIDOQOh"
   },
   "outputs": [],
   "source": [
    "# Обучаем TF-IDF на train, а затем применяем к train и test\n",
    "train_X = vectorizer.fit_transform(train_texts)\n",
    "test_X  = vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lpLQOh1-OQOh",
    "outputId": "e9c2ef8b-100e-4578-cba6-0f09c645d7fc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Пример\n",
    "train_X.todense()[:2] # посмотрим на первые 2 строки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOW0ccLuOQOh"
   },
   "source": [
    "## Обучаем классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cA5nashGOQOi"
   },
   "outputs": [],
   "source": [
    "#import алгоритма из библиотеки\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# инициализируем модель\n",
    "clf = RandomForestClassifier(n_estimators = 500, max_depth = 10)\n",
    "\n",
    "# обучаем ее на тренировочных данных\n",
    "clf = clf.fit(train_X, train_y)\n",
    "\n",
    "# делаем предсказание для тестовых данных\n",
    "pred = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y82Y_60KOQOi",
    "outputId": "e25a120a-c446-47e1-bf97-87a24d96dd12"
   },
   "outputs": [],
   "source": [
    "print('Предсказанные метки: ', pred[0:20], \".....\")\n",
    "print('Истинные метки: ', test_y[0:20], \".....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rAuyFSqOQOi",
    "outputId": "3f18c9bd-24fe-4c0f-b775-a81fdbb588eb"
   },
   "outputs": [],
   "source": [
    "# Функция для кодирования лейблов\n",
    "def num2label(y):\n",
    "    if y == 1:\n",
    "        return 'positive'\n",
    "    if y == -1:\n",
    "        return 'negative'\n",
    "    if y == 0:\n",
    "        return 'neutral'\n",
    "\n",
    "decoded_pred = [num2label(y) for y in pred]\n",
    "decoded_test_y = [num2label(y) for y in test_y]\n",
    "print('Предсказанные метки: ', decoded_pred[0:20], \".....\")\n",
    "print('Истинные метки: ', decoded_test_y [0:20], \".....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvlfKE_zOQOi",
    "outputId": "9df95e61-b08c-4350-abce-23b244b8bee9"
   },
   "outputs": [],
   "source": [
    "print('Accuracy: ', accuracy_score(test_y, pred))\n",
    "print('F1: ', f1_score(test_y, pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTBnYkTMOQOi"
   },
   "source": [
    "## Посмотрим на несколько примеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ns2V-GcJOQOi",
    "outputId": "d5aa5a0c-a9ea-4b29-c7b3-0fd33cc8245b"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('Истинный лейбл:',decoded_test_y[i])\n",
    "    print('Предсказанный лейбл:',decoded_pred[i])\n",
    "    print('Текст новости: ', train_texts[i][:500]+'...')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-eM9AlEOQOi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
